{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use\n",
    "* Calculates the list of all superleaves of length 1-6 tiles (or loads this list in), loads in a log of moves from simulated games, and then calculates the expected value of each superleave based on how much was scored by each rack containing that superleave.\n",
    "\n",
    "#### To-do\n",
    "* The frequency/count estimate for superleaves is currently calculated incorrectly (5/8 - is this still true?)\n",
    "* The synergy calculation is broken.\n",
    "\n",
    "#### Changelog\n",
    "* 5/8/20 - My superleave calculation was being too short-sighted - not considering the future value of keeping a blank on your rack, and failing to recognize the awfulness of gravity wells like UVWW. I added an adjustment factor that tracks the value of your leftover tiles when you make a play from a rack containing that superleave, which will hopefully help with recognizing the value of holding a ? and not holding awful combinations/the Q.\n",
    "* 11/27/19 - wow, it's been awhile. Stopped loading all moves into memory (yikes) and instead wrote a much faster version that can go through 50M moves on my local machine in ~3 hours.\n",
    "* 1/27/19 - Determined that the speed of creation of the rack dataframes is a function of the length of the dataframe. From that, realized that we should organize leaves by least-frequent to most-frequent letter, such that sub-dataframes are created from the shortest racks possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import date\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "from string import ascii_uppercase\n",
    "import time as time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "maximum_superleave_length = 6\n",
    "\n",
    "log_file = '../logs/log_20200514.csv'\n",
    "# log_file = '../logs/log_1m.csv'\n",
    "\n",
    "todays_date = date.today().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20200515'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "todays_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary of all possible 1 to 6-tile leaves. Also, add functionality for sorting by an arbitrary key - allowing us to put rarest letters first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tilebag = ['A']*9+['B']*2+['C']*2+['D']*4+['E']*12+\\\n",
    "#           ['F']*2+['G']*3+['H']*2+['I']*9+['J']*1+\\\n",
    "#           ['K']*1+['L']*4+['M']*2+['N']*6+['O']*8+\\\n",
    "#           ['P']*2+['Q']*1+['R']*6+['S']*4+['T']*6+\\\n",
    "#           ['U']*4+['V']*2+['W']*2+['X']*1+['Y']*2+\\\n",
    "#           ['Z']*1+['?']*2\n",
    "\n",
    "# No superleave is longer than 6 letters, and so we only need to include\n",
    "# 6 each of the As, Es, Is and Os. This shortens the time it takes to find all of\n",
    "# the superleaves by 50%!\n",
    "truncated_tilebag = \\\n",
    "          ['A']*6+['B']*2+['C']*2+['D']*4+['E']*6+\\\n",
    "          ['F']*2+['G']*3+['H']*2+['I']*6+['J']*1+\\\n",
    "          ['K']*1+['L']*4+['M']*2+['N']*6+['O']*6+\\\n",
    "          ['P']*2+['Q']*1+['R']*6+['S']*4+['T']*6+\\\n",
    "          ['U']*4+['V']*2+['W']*2+['X']*1+['Y']*2+\\\n",
    "          ['Z']*1+['?']*2\n",
    "            \n",
    "tiles = [x for x in ascii_uppercase] + ['?']\n",
    "\n",
    "# potential future improvement: calculate optimal order of letters on the fly\n",
    "# rarity_key = 'ZXKJQ?HYMFPWBCVSGDLURTNAOIE'\n",
    "alphabetical_key = '?ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "sort_func = lambda x: alphabetical_key.index(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my home machine, the following code takes about 7 minutes to run in its entirety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0 = time.time()\n",
    "\n",
    "# leaves = {i:sorted(list(set(list(combinations(truncated_tilebag,i))))) for i in \n",
    "#           range(1,maximum_superleave_length+1)}\n",
    "\n",
    "# # turn leaves from lists of letters into strings\n",
    "# # algorithm runs faster if leaves non-alphabetical!\n",
    "# for i in range(1,maximum_superleave_length+1):\n",
    "#     leaves[i] = [''.join(sorted(leave, key=sort_func))\n",
    "#                  for leave in leaves[i]]\n",
    "\n",
    "# t1 = time.time()\n",
    "# print('Calculated superleaves up to length {} in {} seconds'.format(\n",
    "#     maximum_superleave_length,t1-t0))\n",
    "\n",
    "# pkl.dump(leaves,open('all_leaves.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = pkl.load(open('all_leaves.p','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many superleaves are there of each length? See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 27\n",
      "2 373\n",
      "3 3509\n",
      "4 25254\n",
      "5 148150\n",
      "6 737311\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,maximum_superleave_length+1):\n",
    "    print(i,len(leaves[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define metrics we're tallying for each subleaves\n",
    "Currently, we track the following metrics with each new rack:\n",
    "* Total points\n",
    "* Count (how many times subleaves has appeared in data set)\n",
    "* Bingo Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_leaves = []\n",
    "\n",
    "for i in range(1,maximum_superleave_length+1):\n",
    "    all_leaves.extend(leaves[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subleaves(rack, min_length=1, max_length=6, duplicates_allowed = False):\n",
    "    if not duplicates_allowed:\n",
    "        return [''.join(sorted(x, key=sort_func)) for i in range(min_length, max_length+1) \n",
    "            for x in set(list(combinations(rack,i)))]\n",
    "    else:\n",
    "        return [''.join(sorted(x, key=sort_func)) for i in range(min_length, max_length+1) \n",
    "            for x in list(combinations(rack,i))]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*tile_limit* below is the minimum number of tiles left on a rack for it to be factored into superleave calculation. The idea is that moves with the bag empty tend to be worth less, and may not reflect the value of a letter in the rest of the game (most notably, if you have the blank and the bag is empty, you often can't bingo!). Moves are tend to be worth a little bit less at the beginning of a game when there are fewer juicy spots to play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 rows in 0.6810328960418701 seconds\n",
      "Processed 1000000 rows in 299.34818387031555 seconds\n",
      "Processed 2000000 rows in 567.5604648590088 seconds\n",
      "Processed 3000000 rows in 837.5055487155914 seconds\n",
      "Processed 4000000 rows in 1107.6123669147491 seconds\n",
      "Processed 5000000 rows in 1378.0669567584991 seconds\n",
      "Processed 6000000 rows in 1652.3204958438873 seconds\n",
      "Processed 7000000 rows in 1923.1418929100037 seconds\n",
      "Processed 8000000 rows in 2194.7655749320984 seconds\n",
      "Processed 9000000 rows in 2466.54762673378 seconds\n",
      "Processed 10000000 rows in 2738.933168888092 seconds\n",
      "Processed 11000000 rows in 3011.0271508693695 seconds\n",
      "Processed 12000000 rows in 3283.3966426849365 seconds\n",
      "Processed 13000000 rows in 3557.1026887893677 seconds\n",
      "Processed 14000000 rows in 3830.2991468906403 seconds\n",
      "Processed 15000000 rows in 4103.08988070488 seconds\n",
      "Processed 16000000 rows in 4375.417726993561 seconds\n",
      "Processed 17000000 rows in 4648.062791824341 seconds\n",
      "Processed 18000000 rows in 4920.394510746002 seconds\n",
      "Processed 19000000 rows in 5193.406304836273 seconds\n",
      "Processed 20000000 rows in 5467.246378660202 seconds\n",
      "Processed 21000000 rows in 5740.596211910248 seconds\n",
      "Processed 22000000 rows in 6014.014837026596 seconds\n",
      "Processed 23000000 rows in 6287.4219217300415 seconds\n",
      "Processed 24000000 rows in 6560.28736782074 seconds\n",
      "Processed 25000000 rows in 6834.181646823883 seconds\n",
      "Processed 26000000 rows in 7107.872599840164 seconds\n",
      "Processed 27000000 rows in 7382.129773855209 seconds\n",
      "Processed 28000000 rows in 7656.279711961746 seconds\n",
      "Processed 29000000 rows in 7929.651953935623 seconds\n",
      "Processed 30000000 rows in 8203.497105836868 seconds\n",
      "Processed 31000000 rows in 8477.394650936127 seconds\n",
      "Processed 32000000 rows in 8750.967122793198 seconds\n",
      "Processed 33000000 rows in 9025.067343950272 seconds\n",
      "Processed 34000000 rows in 9299.159978628159 seconds\n",
      "Processed 35000000 rows in 9572.876962900162 seconds\n",
      "Processed 36000000 rows in 9846.417187690735 seconds\n",
      "Processed 37000000 rows in 10120.158997774124 seconds\n",
      "Processed 38000000 rows in 10394.029653072357 seconds\n",
      "Processed 39000000 rows in 10668.147599935532 seconds\n",
      "Processed 40000000 rows in 10943.094893693924 seconds\n",
      "Processed 41000000 rows in 11217.060456991196 seconds\n",
      "Processed 42000000 rows in 11491.015597820282 seconds\n",
      "Processed 43000000 rows in 11765.485030889511 seconds\n",
      "Processed 44000000 rows in 12039.60313987732 seconds\n",
      "Processed 45000000 rows in 12313.831724882126 seconds\n",
      "Processed 46000000 rows in 12587.828927993774 seconds\n",
      "Processed 47000000 rows in 12862.040288686752 seconds\n",
      "Processed 48000000 rows in 13136.361006736755 seconds\n",
      "Processed 49000000 rows in 13410.214724779129 seconds\n",
      "Processed 50000000 rows in 13684.334517717361 seconds\n",
      "Processed 51000000 rows in 13958.59805393219 seconds\n",
      "Processed 52000000 rows in 14233.195499658585 seconds\n",
      "Processed 53000000 rows in 14507.859705924988 seconds\n",
      "Processed 54000000 rows in 14782.191229820251 seconds\n",
      "Processed 55000000 rows in 15056.281200885773 seconds\n",
      "Processed 56000000 rows in 15330.481600761414 seconds\n",
      "Processed 57000000 rows in 15604.499203920364 seconds\n",
      "Processed 58000000 rows in 15878.553917884827 seconds\n",
      "Processed 59000000 rows in 16152.571445703506 seconds\n",
      "Processed 60000000 rows in 16426.815789699554 seconds\n",
      "Processed 61000000 rows in 16700.991994857788 seconds\n",
      "Processed 62000000 rows in 16975.83785390854 seconds\n",
      "Processed 63000000 rows in 17250.70567393303 seconds\n",
      "Processed 64000000 rows in 17524.833360910416 seconds\n",
      "Processed 65000000 rows in 17799.182710647583 seconds\n",
      "Processed 66000000 rows in 18073.674301862717 seconds\n",
      "Processed 67000000 rows in 18348.15354990959 seconds\n",
      "Processed 68000000 rows in 18622.52593779564 seconds\n",
      "Processed 69000000 rows in 18897.256729841232 seconds\n",
      "Processed 70000000 rows in 19171.18041586876 seconds\n",
      "Processed 71000000 rows in 19445.165927886963 seconds\n",
      "Processed 72000000 rows in 19719.577728033066 seconds\n",
      "Processed 73000000 rows in 19993.713971853256 seconds\n",
      "Processed 74000000 rows in 20269.029147863388 seconds\n",
      "Processed 75000000 rows in 20543.18798184395 seconds\n",
      "Processed 76000000 rows in 20817.71434378624 seconds\n",
      "Processed 77000000 rows in 21092.06179189682 seconds\n",
      "Processed 78000000 rows in 21367.21520590782 seconds\n",
      "Processed 79000000 rows in 21641.01642870903 seconds\n",
      "Processed 80000000 rows in 21915.93760085106 seconds\n",
      "Processed 81000000 rows in 22190.057852983475 seconds\n",
      "Processed 82000000 rows in 22464.806837797165 seconds\n",
      "Processed 83000000 rows in 22739.929269075394 seconds\n",
      "Processed 84000000 rows in 23014.11400079727 seconds\n",
      "Processed 85000000 rows in 23288.556773900986 seconds\n",
      "Processed 86000000 rows in 23563.792271852493 seconds\n",
      "Processed 87000000 rows in 23838.74466776848 seconds\n",
      "Processed 88000000 rows in 24113.12270975113 seconds\n",
      "Processed 89000000 rows in 24387.873302698135 seconds\n",
      "Processed 90000000 rows in 24662.164838790894 seconds\n",
      "Processed 91000000 rows in 24937.219473838806 seconds\n",
      "Processed 92000000 rows in 25213.429934740067 seconds\n",
      "Processed 93000000 rows in 25488.7355158329 seconds\n",
      "Processed 94000000 rows in 25764.826680898666 seconds\n",
      "Processed 95000000 rows in 26040.4468998909 seconds\n",
      "Processed 96000000 rows in 26315.733648777008 seconds\n",
      "Processed 97000000 rows in 26591.10465168953 seconds\n",
      "Processed 98000000 rows in 26866.1714220047 seconds\n",
      "Processed 99000000 rows in 27140.606106758118 seconds\n",
      "Processed 100000000 rows in 27415.640650987625 seconds\n",
      "Processed 101000000 rows in 27690.204962730408 seconds\n",
      "Processed 102000000 rows in 27965.768276929855 seconds\n",
      "Processed 103000000 rows in 28239.69007086754 seconds\n",
      "Processed 104000000 rows in 28514.152144908905 seconds\n",
      "Processed 105000000 rows in 28788.34059882164 seconds\n",
      "Processed 106000000 rows in 29063.336795806885 seconds\n",
      "Processed 107000000 rows in 29338.307721853256 seconds\n",
      "Processed 108000000 rows in 29612.78927373886 seconds\n",
      "Processed 109000000 rows in 29888.255115032196 seconds\n",
      "Processed 110000000 rows in 30162.884897708893 seconds\n",
      "Processed 111000000 rows in 30437.767520666122 seconds\n",
      "Processed 112000000 rows in 30712.218968868256 seconds\n",
      "30721.909772872925 seconds to populate dictionaries\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "tile_limit = 1\n",
    "\n",
    "bingo_count = {x:0 for x in all_leaves}\n",
    "count = {x:0 for x in all_leaves}\n",
    "equity = {x:0 for x in all_leaves}\n",
    "points = {x:0 for x in all_leaves}\n",
    "row_count = 0\n",
    "total_equity = 0\n",
    "total_points = 0\n",
    "\n",
    "with open(log_file,'r') as f:\n",
    "    moveReader = csv.reader(f)\n",
    "    next(moveReader)\n",
    "    \n",
    "    for i,row in enumerate(moveReader):\n",
    "        if i%1000000==0:\n",
    "            t = time.time()\n",
    "            print('Processed {} rows in {} seconds'.format(i,t-t0))\n",
    "        \n",
    "#         if i<10:\n",
    "#             print(i,row)\n",
    "            \n",
    "        try:    \n",
    "            if int(row[10]) >= tile_limit:\n",
    "\n",
    "                total_equity += float(row[9])\n",
    "                total_points += int(row[5])\n",
    "                row_count += 1\n",
    "\n",
    "                for subleave in find_subleaves(row[3],\n",
    "                        max_length=maximum_superleave_length):\n",
    "                    bingo_count[subleave] += row[7] == '7'\n",
    "                    count[subleave] += 1\n",
    "                    equity[subleave] += float(row[9])\n",
    "                    points[subleave] += int(row[5])\n",
    "        except:\n",
    "            print(i,row)\n",
    "\n",
    "t1 = time.time()\n",
    "print('{} seconds to populate dictionaries'.format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_df = pd.concat([pd.Series(points, name='points'),\n",
    "                  pd.Series(equity, name='equity'),\n",
    "                  pd.Series(count, name='count'),\n",
    "                  pd.Series(bingo_count, name='bingo_count')],\n",
    "                  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score = total_points/row_count\n",
    "mean_equity = total_equity/row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_df['mean_score'] = ev_df['points']/ev_df['count']\n",
    "ev_df['mean_equity'] = ev_df['equity']/ev_df['count']\n",
    "ev_df['bingo pct'] = 100*ev_df['bingo_count']/ev_df['count']\n",
    "ev_df['pct'] = 100*ev_df['count']/len(ev_df)\n",
    "ev_df['adjusted_mean_score'] = ev_df['mean_score']-mean_score\n",
    "ev_df['ev'] = ev_df['mean_equity']-mean_equity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoothed superleaves\n",
    "We calculate superleaves by having Macondo play itself millions of times, and then seeing the difference between how much plays score that contain that superleave versus the average of all other plays (the \"leave value\"). However, some of the lower probability superleaves get observed very infrequently, and so end up with inaccurate superleave values (for instance, if the one time you have DLPQX? you played QUADPLEX for 300+, you're going to incorrectly think that's a dream leave!).\n",
    "\n",
    "To compensate this, we \"smooth out\" the superleaves for any superleave that was observed less than a cutoff number of times (maybe 50 or 100). We sum up over the statistics for all neighboring leaves (all leaves that are only different by 1 tile and contain the same number of blanks). The proper way of doing this is really with a superior model like a neural net, but this gets pretty close and prevents \"gravity wells\" (when a superleave is valued way too high, and the fast player will keep trying to keep that superleave at all costs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['leave'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-4d469addd738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# avoid tampering with ev_df above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m summary_df = pd.read_csv('leave_summary_' + todays_date +'.csv').rename(\n\u001b[0;32m----> 3\u001b[0;31m     columns={'Unnamed: 0':'leave'}).set_index('leave')\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mset_index\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   4394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4396\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None of {} are in the columns\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of ['leave'] are in the columns\""
     ]
    }
   ],
   "source": [
    "# avoid tampering with ev_df above\n",
    "summary_df = pd.read_csv('leave_summary_' + todays_date +'.csv').rename(\n",
    "    columns={'Unnamed: 0':'leave'}).set_index('leave')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = summary_df['count'].to_dict()\n",
    "equity_dict = summary_df['equity'].to_dict()\n",
    "mean_equity_dict = summary_df['mean_equity'].to_dict()\n",
    "summary_df = summary_df.reset_index()\n",
    "summary_df['leave_len'] = summary_df['leave'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_leaves = {leave:[''.join(sorted(leave+letter, key=sort_func)) for letter in alphabetical_key]\n",
    "                for i in range(1,6) for leave in leaves[i]}\n",
    "child_leaves[''] = [x for x in alphabetical_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighboring_leaves(original_leave):\n",
    "    t0 = time.time()\n",
    "    subleaves = [''.join(x) for x in combinations(original_leave, len(original_leave)-1)]\n",
    "    t1 = time.time()\n",
    "    \n",
    "    neighbors = []\n",
    "    for leave in subleaves:\n",
    "        neighbors += child_leaves[leave]\n",
    "        \n",
    "    t2 = time.time()\n",
    "    \n",
    "    # filter neighbors to make sure they have the same number of blanks\n",
    "    blank_count = sum([x=='?' for x in original_leave])\n",
    "    \n",
    "    t3 = time.time()\n",
    "    \n",
    "    neighbors = [leave for leave in neighbors if(sum([x=='?' for x in leave])==blank_count)]\n",
    "    \n",
    "    t4 = time.time()\n",
    "    \n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def calculate_smoothed_superleave(superleave):\n",
    "    neighbors = get_neighboring_leaves(superleave)\n",
    "    \n",
    "    neighboring_equity = 0\n",
    "    neighboring_count = 0\n",
    "    equity_list = []\n",
    "        \n",
    "    for neighbor_leave in neighbors:\n",
    "        neighboring_equity += equity_dict.get(neighbor_leave, 0)\n",
    "        neighboring_count += count_dict.get(neighbor_leave, 0)\n",
    "        equity_list.append(mean_equity_dict.get(neighbor_leave))\n",
    "                \n",
    "    equity_list = [x for x in equity_list if pd.notnull(x)]\n",
    "    \n",
    "#     print('Original:')\n",
    "#     print(summary_df.loc[summary_df['leave']==superleave])\n",
    "#     print(neighboring_equity, neighboring_count, neighboring_equity/neighboring_count)\n",
    "#     print(np.mean(equity_list))\n",
    "#     print(equity_list)\n",
    "    \n",
    "    return neighboring_equity/neighboring_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows how many 6-tile superleaves were never seen, and how many were seen less than 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.notnull(summary_df.loc[summary_df['leave_len']==5])['ev'].value_counts())\n",
    "print((summary_df.loc[summary_df['leave_len']==5]['count']<10).value_counts())\n",
    "\n",
    "print(pd.notnull(summary_df.loc[summary_df['leave_len']==6])['ev'].value_counts())\n",
    "print((summary_df.loc[summary_df['leave_len']==6]['count']<10).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the strongest superleaves in your lexicon. If your superleaves are unsmoothed, you'll likely see some weird superleaves at the top of this list with low count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.loc[summary_df['leave_len']==5].sort_values('ev', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df['smoothed_ev'] = summary_df['ev']\n",
    "summary_df['point_equity_diff'] = (summary_df['points']-summary_df['equity'])/summary_df['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there's a big delta between the equity scored with a given leave and the average points, that can be a sign that your existing ev for a superleave is too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.loc[summary_df['leave_len']==5].sort_values('point_equity_diff')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the minimum number of times you want to see a superleave before you'll take the\n",
    "# value as is, without smoothing?\n",
    "five_tile_superleave_cutoff = 100\n",
    "six_tile_superleave_cutoff = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'leave'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'leave'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-45a70c007743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m leaves_to_smooth = list(summary_df.loc[(summary_df['leave_len']==5) & \n\u001b[0;32m----> 2\u001b[0;31m     (summary_df['count']<five_tile_superleave_cutoff)]['leave'].values)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaves_to_smooth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m leaves_to_smooth += list(summary_df.loc[(summary_df['leave_len']==6) &\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2978\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2979\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2980\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'leave'"
     ]
    }
   ],
   "source": [
    "leaves_to_smooth = list(summary_df.loc[(summary_df['leave_len']==5) & \n",
    "    (summary_df['count']<five_tile_superleave_cutoff)]['leave'].values)\n",
    "print(len(leaves_to_smooth))\n",
    "\n",
    "leaves_to_smooth += list(summary_df.loc[(summary_df['leave_len']==6) &\n",
    "    (summary_df['count']<six_tile_superleave_cutoff)]['leave'].values)\n",
    "print(len(leaves_to_smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = summary_df.set_index('leave')\n",
    "smooth_ev_dict = summary_df['ev'].to_dict()\n",
    "ev_dict = summary_df['ev'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "for i,leave in enumerate(leaves_to_smooth):\n",
    "    if (i+1)%1000==0:\n",
    "        print(i, time.time()-t0)\n",
    "    \n",
    "    smooth_ev_dict[leave] = calculate_smoothed_superleave(leave) - mean_equity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_ev_series = pd.Series(smooth_ev_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate rack synergy\n",
    "In other words, how much better is the EV of this superleave than the value of each tile on its own? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated \"synergy\" in 1.1174757480621338 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "synergy = {leave: smooth_ev_dict[leave]-sum([smooth_ev_dict[letter] for letter in leave]) \n",
    "    for leave in all_leaves}\n",
    "        \n",
    "t1 = time.time()\n",
    "print('Calculated synergy in {} seconds'.format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_df = pd.concat([ev_df,pd.Series(synergy, name='synergy')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save superleaves to an external file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "ev_df['ev'].to_csv('leave_values_' + todays_date + '_unsmoothed.csv')\n",
    "ev_df.reset_index().to_csv('leave_summary_' + todays_date + '.csv', index=False)\n",
    "smooth_ev_series.to_csv('leave_values_' + todays_date + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "ev_df['synergy'].sort_values().to_csv('leave_synergies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "ev_df['synergy'].sort_values(ascending=False)[:100].to_csv('leave_synergies_top100.csv')\n",
    "ev_df['synergy'].sort_values(ascending=True)[:100].to_csv('leave_synergies_bottom100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
